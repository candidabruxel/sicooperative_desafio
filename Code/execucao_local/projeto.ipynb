{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://postgres:admin@localhost:5432/desafio\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DB_USERNAME = 'postgres'\n",
    "DB_PASSWORD = 'admin'\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '5432'\n",
    "DB_DATABASE = 'desafio'\n",
    "\n",
    "# Conexão usando SQLAlchemy\n",
    "string_conexao = f\"postgresql+psycopg2://{DB_USERNAME}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_DATABASE}\"\n",
    "\n",
    "# Tente imprimir a string de conexão para verificar se ela está correta\n",
    "print(string_conexao)\n",
    "\n",
    "# Crie uma engine usando a string de conexão\n",
    "engine = create_engine(string_conexao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id     nome sobrenome  idade                       email\n",
      "0   1   Carlos  Ferreira     28   carlos.ferreira@email.com\n",
      "1   2      Ana  Oliveira     35      ana.oliveira@email.com\n",
      "2   3    Pedro   Almeida     40     pedro.almeida@email.com\n",
      "3   4  Mariana     Costa     22     mariana.costa@email.com\n",
      "4   5  Ricardo  Silveira     33  ricardo.silveira@email.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Teste de leitura\n",
    "consulta_sql = \"SELECT * FROM associado\"\n",
    "dados_tabela = pd.read_sql(consulta_sql, engine)\n",
    "#caminho_arquivo_csv = 'C:/Users/elton/Documents/projeto/sicooperative/sicooperative/Code/Development'\n",
    "#dados_tabela.to_csv(caminho_arquivo_csv, index=False)\n",
    "# Exiba os dados\n",
    "print(dados_tabela.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do Spark com o driver JDBC do PostgreSQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL do Banco de Dados\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:/Users/elton/Documents/projeto/postgresql-42.7.0.jar\") \\\n",
    "    .config(\"spark.hadoop.home.dir\", \"C:/Users/elton/Documents/spark-3.5.0-bin-hadoop3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão do Spark criada com sucesso!\n",
      "Versão do Spark: 3.5.0\n",
      "Classpath do Spark: C:/Users/elton/Documents/projeto/postgresql-42.7.0.jar\n",
      "spark.app.id=local-1701122926953\n",
      "spark.app.name=ETL do Banco de Dados\n",
      "spark.app.startTime=1701122918896\n",
      "spark.app.submitTime=1701122915417\n",
      "spark.driver.extraClassPath=C:/Users/elton/Documents/projeto/postgresql-42.7.0.jar\n",
      "spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host=DESKTOP-V5VK3N2.unimedvtrp.com.br\n",
      "spark.driver.port=62324\n",
      "spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id=driver\n",
      "spark.hadoop.home.dir=C:/Users/elton/Documents/spark-3.5.0-bin-hadoop3\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n"
     ]
    }
   ],
   "source": [
    "# Imprime informações sobre a sessão do Spark\n",
    "print(\"Sessão do Spark criada com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(\"Classpath do Spark:\", spark._jsc.sc().getConf().get(\"spark.driver.extraClassPath\"))\n",
    "print(spark.sparkContext.getConf().toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de conexão JDBC\n",
    "url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_DATABASE}\"\n",
    "\n",
    "# Propriedades para a conexão JDBC\n",
    "properties = {\n",
    "    \"user\": DB_USERNAME,\n",
    "    \"password\": DB_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leitura da tabela associado como um DataFrame (Zona Bruta)\n",
    "df_associado_raw = spark.read.jdbc(url=url, table=\"associado\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+-----+--------------------+\n",
      "| id|    nome|sobrenome|idade|               email|\n",
      "+---+--------+---------+-----+--------------------+\n",
      "|  1|  Carlos| Ferreira|   28|carlos.ferreira@e...|\n",
      "|  2|     Ana| Oliveira|   35|ana.oliveira@emai...|\n",
      "|  3|   Pedro|  Almeida|   40|pedro.almeida@ema...|\n",
      "|  4| Mariana|    Costa|   22|mariana.costa@ema...|\n",
      "|  5| Ricardo| Silveira|   33|ricardo.silveira@...|\n",
      "|  6|  Camila|  Santana|   29|camila.santana@em...|\n",
      "|  7|   Lucas|  Pereira|   31|lucas.pereira@ema...|\n",
      "|  8|   Aline|  Martins|   26|aline.martins@ema...|\n",
      "|  9|Fernando|  Ribeiro|   37|fernando.ribeiro@...|\n",
      "| 10| Juliana|     Lima|   24|juliana.lima@emai...|\n",
      "+---+--------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_associado_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o DataFrame como Delta Lake\n",
    "#df_associado_raw.write.format(\"delta\").mode(\"overwrite\").save(caminho_raw_zone + \"/associado\")\n",
    "\n",
    "# Converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "stg_associado = df_associado_raw.toPandas()\n",
    "\n",
    "# Escreva o DataFrame do Pandas em formato Parquet\n",
    "caminho_raw_zone = \"stg_associado.parquet\"\n",
    "stg_associado.to_parquet(caminho_raw_zone, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura da tabela conta como um DataFrame (Zona Bruta)\n",
    "df_conta_raw = spark.read.jdbc(url=url, table=\"conta\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "stg_conta = df_conta_raw.toPandas()\n",
    "\n",
    "# Escreva o DataFrame do Pandas em formato Parquet\n",
    "caminho_raw_zone = \"stg_conta.parquet\"\n",
    "stg_conta.to_parquet(caminho_raw_zone, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura da tabela cartao como um DataFrame (Zona Bruta)\n",
    "df_cartao_raw = spark.read.jdbc(url=url, table=\"cartao\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "stg_cartao = df_cartao_raw.toPandas()\n",
    "\n",
    "# Escreva o DataFrame do Pandas em formato Parquet\n",
    "caminho_raw_zone = \"stg_cartao.parquet\"\n",
    "stg_cartao.to_parquet(caminho_raw_zone, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura da tabela movimento como um DataFrame (Zona Bruta)\n",
    "df_movimento_raw = spark.read.jdbc(url=url, table=\"movimento\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "stg_movimento = df_movimento_raw.toPandas()\n",
    "\n",
    "# Escreva o DataFrame do Pandas em formato Parquet\n",
    "caminho_raw_zone = \"stg_movimento.parquet\"\n",
    "stg_movimento.to_parquet(caminho_raw_zone, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL para criação da tabela movimento_flat)\n",
    "df_dim_associado = spark.read.parquet(\"stg_associado.parquet\")\n",
    "df_dim_conta = spark.read.parquet(\"stg_conta.parquet\")\n",
    "df_dim_cartao = spark.read.parquet(\"stg_cartao.parquet\")\n",
    "df_dim_movimento = spark.read.parquet(\"stg_movimento.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movimento_flat = df_dim_associado.alias(\"associado\") \\\n",
    "    .join(df_dim_conta.alias(\"conta\"), col(\"associado.id\") == col(\"conta.id_associado\")) \\\n",
    "    .join(df_dim_cartao.alias(\"cartao\"), col(\"conta.id\") == col(\"cartao.id_conta\")) \\\n",
    "    .join(df_dim_movimento.alias(\"movimento\"), col(\"cartao.id\") == col(\"movimento.id_cartao\")) \\\n",
    "    .select(\n",
    "        col(\"associado.nome\").alias(\"nome_associado\"),\n",
    "        col(\"associado.sobrenome\").alias(\"sobrenome_associado\"),\n",
    "        col(\"associado.idade\").alias(\"idade_associado\"),\n",
    "        col(\"movimento.vls_transacao\").alias(\"vlr_transacao_movimento\"),\n",
    "        col(\"movimento.des_transacao\").alias(\"des_transacao_movimento\"),\n",
    "        col(\"movimento.data_movimento\"),\n",
    "        col(\"cartao.num_cartao\").alias(\"numero_cartao\"),\n",
    "        col(\"cartao.nom_impresso\").alias(\"nome_impresso_cartao\"),\n",
    "        col(\"conta.tipo\"),\n",
    "        col(\"conta.data_criacao\").alias(\"data_criacao_conta\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------------+-----------------------+-----------------------+-------------------+-------------+--------------------+------------+-------------------+\n",
      "|nome_associado|sobrenome_associado|idade_associado|vlr_transacao_movimento|des_transacao_movimento|     data_movimento|numero_cartao|nome_impresso_cartao|        tipo| data_criacao_conta|\n",
      "+--------------+-------------------+---------------+-----------------------+-----------------------+-------------------+-------------+--------------------+------------+-------------------+\n",
      "|         Pedro|            Almeida|             40|                 150.75|           Supermercado|2023-04-10 00:00:00|        11112|       Pedro Almeida|Investimento|2023-02-10 00:00:00|\n",
      "|       Mariana|              Costa|             22|                  45.80|            Restaurante|2023-04-15 00:00:00|        55556|       Mariana Costa|    Corrente|2023-02-15 00:00:00|\n",
      "|       Ricardo|           Silveira|             33|                  60.00|     Compra eletronicos|2023-04-20 00:00:00|        99990|    Ricardo Silveira|    Corrente|2023-02-20 00:00:00|\n",
      "|        Camila|            Santana|             29|                  25.30|               Farmacia|2023-04-25 00:00:00|        33334|      Camila Santana|Investimento|2023-02-25 00:00:00|\n",
      "|         Lucas|            Pereira|             31|                  80.45|               Livraria|2023-05-01 00:00:00|        77778|       Lucas Pereira|    Corrente|2023-03-01 00:00:00|\n",
      "|         Aline|            Martins|             26|                  55.60|               Shopping|2023-05-05 00:00:00|        44445|       Aline Martins|    Corrente|2023-03-05 00:00:00|\n",
      "|      Fernando|            Ribeiro|             37|                  90.20|                 Viagem|2023-05-10 00:00:00|        88889|    Fernando Ribeiro|Investimento|2023-03-10 00:00:00|\n",
      "|       Juliana|               Lima|             24|                  40.75|                 Cinema|2023-05-15 00:00:00|        22223|        Juliana Lima|    Corrente|2023-03-15 00:00:00|\n",
      "|        Carlos|           Ferreira|             28|                  75.20|          Compra online|2023-04-01 00:00:00|        12345|     Carlos Ferreira|    Corrente|2023-03-20 00:00:00|\n",
      "|           Ana|           Oliveira|             35|                  30.50|      Posto de gasolina|2023-04-05 00:00:00|        98765|        Ana Oliveira|Investimento|2023-03-25 00:00:00|\n",
      "+--------------+-------------------+---------------+-----------------------+-----------------------+-------------------+-------------+--------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movimento_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converta o DataFrame do Spark para um DataFrame do Pandas\n",
    "fato_movimento_flat = df_movimento_flat.toPandas()\n",
    "# Escreva o DataFrame do Pandas em formato Parquet\n",
    "caminho_raw_zone = \"movimento_flat.parquet\"\n",
    "fato_movimento_flat.to_parquet(caminho_raw_zone, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreva o DataFrame do Pandas em formato CSV\n",
    "caminho_raw_zone_csv = \"movimento_flat.csv\"\n",
    "fato_movimento_flat.to_csv(caminho_raw_zone_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaliza a sessão do Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
